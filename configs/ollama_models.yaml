# --- GLOBAL SETTINGS ---

OLLAMA_BASE_URL: "http://localhost:11434"

# --- EMBEDDING MODEL (For RAG/ChromaDB) ---

EMBEDDING_MODEL:
  model: "nomic-embed-text"
  base_url: "http://localhost:11434"

# --- AGENT-SPECIFIC LLM SETTINGS ---
AGENT_MODELS:
  Orchestrator:
    model: "qwen2.5:7b"
    temperature: 0.0
    system_prompt_file: "configs/system_prompts/orchestrator_prompt.txt"
    num_ctx: 65536          
    num_predict: 4096       
    format: "json"
    enable_streaming: false

  Professor:
    model: "qwen2.5:7b"
    temperature: 0.1
    system_prompt_file: "configs/system_prompts/professor_prompt.txt"
    num_ctx: 65536         
    num_predict: 4096      
    format: ""
    tools: [search_memory, search_documents]
    enable_streaming: true

  Researcher:
    model: "qwen2.5:7b"
    temperature: 0.3
    system_prompt_file: "configs/system_prompts/researcher_prompt.txt"
    num_ctx: 65536         
    num_predict: 4096      
    format: ""
    tools: [search_memory, tavily_search_api, tavily_extract_content]
    enable_streaming: true

  Communicator:
    model: "qwen2.5:7b"
    temperature: 0.2
    system_prompt_file: "configs/system_prompts/communicator_prompt.txt"
    num_ctx: 65536         
    num_predict: 4096      
    format: ""
    tools: [search_memory, get_emails, gmail_send_message]
    enable_streaming: true

  Secretary:
    model: "qwen2.5:7b"
    temperature: 0.1
    system_prompt_file: "configs/system_prompts/secretary_prompt.txt"
    num_ctx: 65536         
    num_predict: 4096      
    format: ""
    tools: [search_memory, search_calendar_events, create_calendar_event, update_calendar_event, delete_calendar_event]
    enable_streaming: true

  Accountant:
    model: "qwen2.5:7b"
    temperature: 0.05
    system_prompt_file: "configs/system_prompts/accountant_prompt.txt"
    num_ctx: 65536          
    num_predict: 4096       
    format: ""
    tools: [search_memory, add_transaction, get_recent_transactions, execute_sql_write, sql_list_tables, sql_get_schema, sql_query, sql_query_checker]
    enable_streaming: true

  Responder:
    model: "qwen2.5:7b"
    temperature: 0.6
    system_prompt_file: "configs/system_prompts/responder_prompt.txt"
    num_ctx: 65536         
    num_predict: 4096      
    format: ""
    tools: [search_memory]
    enable_streaming: true

  Distiller:
    model: "qwen2.5:7b"
    temperature: 0.2
    system_prompt_file: "configs/system_prompts/distiller_prompt.txt"
    num_ctx: 8192         
    num_predict: 128      
    format: ""
    enable_streaming: false